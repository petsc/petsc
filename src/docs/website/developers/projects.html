<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html;charset=utf-8">
    <link href="../style.css" rel="stylesheet" type="text/css">
    <title>PETSc: The Portable, Extensible Toolkit for Scientific Computation</title>
  </head>
  <body>

    <div id="logo">
      <h1>PETSc</h1>
    </div>

    <div id="header">
      <h1>Projects: Big and Small</h1>
    </div>

    <hr>

    <div id="sidebar">
      <ul>
        <li><a href="../index.html">Home</a></li>
        <li><a href="../download/index.html">Download</a></li>
        <li><a href="../features/index.html">Features</a></li>
        <li><a href="../documentation/index.html">Documentation</a></li>
        <li><a href="../publications/index.html">Applications/Publications</a></li>
        <li><a href="../miscellaneous/index.html">Miscellaneous</a></li>
        <li><a href="../miscellaneous/external.html">External Software</a></li>
        <li>
          <span class="current">Developers Site</span>
          <ul>
            <li><a href="index.html">Getting started</a></li>
            <li><a href="../documentation/changes/dev.html">Changes</a></li>
            <li><a href="http://www.mcs.anl.gov/petsc/petsc-dev/docs/index.html">Development copy of documentation</a></li>
            <li><a href="developers.pdf">Developers Instructions</a></li>
            <li><span class="current">Proposed projects</span></li>
          </ul>
        </li>
      </ul>
    </div>

    <div id="main">

      This is the PETSc to-do list; any help on these would be greatly
      appreciated. Each of the first few projects could be done by a graduate
      student or advanced undergraduate in a couple of months.

      <ul>
        <li>When PETSc loads a matrix rank 0 first loads its piece of data, then loads data to be sent to other processes.
This algorithm basically causes that rank 0 uses twice the memory it needs, at least for a while.
This is sometimes a problem, mainly for big matrices. If you know that a piece of your matrix will be safely stored on one node, sometimes it happens that a single node cannot keep twice its piece of the matrix.
If MatLoad don't immediately load the data for rank 0 and they only store the file offset to the beginning of data for rank 0, they could start loading one by one the blocks of data to be sent to other processes. Only at the end, they will return to the saved offset and load the piece for the rank 0.
        </li>

        <li>MatSOR() should used blocks when bs > 0 for AIJ matrices and no inodes        </li>

        <li>
          Daniel Szyld suggests a refined way of computing overlap for additive
          Schwarz. Instead of taking all rows/columns connected to the block
          take those with "strong" coupling. I think this could be easily added
          to PETSc.
        </li>

        <li> Add optional scalar value argument to VecMAXPY(), fix all places in code that depend on it.  </li>

        <li>
          Develop the parallel PETSc interface for the new FFTW parallel codes
          being developed, based on the sequential interfaces in
          src/mat/impls/fftw
        </li>

        <li>
          Developing fast (FFT based) solvers (underneath src/ksp/pc/impls for
          "the model problems", sort of like a modern FISHPACK in PETSc. For
          parallel this requires the FFTW parallel interface.
        </li>

        <li>
          Converting PetscLogViewPython() to generate JSON instead and
          developing Python parsers for quickly generating nice tables of
          performance details from runs or groups of runs.
        </li>

        <li>
          Extend PetscWebServe() that allows accessing running PETSc jobs via
          a browser and looking at what is going on (this uses the AMS inside).
          See src/sys/viewer/impls/socket/send.c
        </li>

        <li>GPU based preconditiones </li>
        <li>Add PCApplyHermitianTranpose() and KSPSolveHermitianTranspose()</li>
        <li>Interface to CUBLAS for dense linear algebra on GPUs</li>

        <li>
          Improve the PETSc website so that searchs for, for example
          MatGetSubMatrix() at google always go to the latest PETSc docs and
          not some out of date stuff on another site. See
          <a href="http://www.google.com/support/webmasters/bin/answer.py?answer=183668">here</a>.
        </li>

        <li>
          Interface to
          <a href="http://code.google.com/p/elemental/">Elemental</a> for modern
          distributed-memory dense linear algebra
        </li>

        <li>Vec and Mat class based on pthreads parallelism</li>

        <li>
          Turn MPICH, Open MPI and then PETSc into an Apple Framework.
        </li>

        <li>
          Make a Mat subclass for tridiagonal matrix (and block tridiagonal)
          and write fast sequential solvers for them. Also do fast parallel
          tridiagonal solvers.
        </li>

        <li>
          Convert KSPSPECEST to something like -ksp_type chebychev
          -ksp_chebychev_precompute_parameters
          -ksp_chebychev_precompute_ksp_type cg
          -ksp_chebychev_precompute_minfactor .95
        </li>

        <li>MatSOR() should used blocks when bs > for AIJ matrices and no inodes        </li>

        <li>
          KSPSolve and SNESSolve should return bounds on the errors and some
          measure of sensitivity of the solution to the something
        </li>

        <li>
          The MatCreateSeqCUFFT() and MatCreateSeqFFTW() should be both made
          subclasses of an abstract Mat class for FFTs. Allowing command line
          switching between them etc so that user code makes no mention of the
          particular FFT package being used.  
        </li>

        <li>
          Remove all mention of Sean from the PETSc repositories.
        </li>

      </ul>

    </div>

    <hr>

  </body>
</html>
