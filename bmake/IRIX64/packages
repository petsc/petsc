# $Id: base.site,v 1.57 1999/10/04 22:25:08 balay Exp balay $ 

#  This file contains site-specific information.  The definitions below
#  should be changed to match the locations of libraries at your site.
#  The following naming convention is used:
#     XXX_LIB - location of library XXX
#     XXX_INCLUDE - directory for include files needed for library XXX
#
# Location of BLAS and LAPACK. See ${BS_DIR}/docs/installation.html for
# information on retrieving them.
#
# BLAS usually comes with SGI. Do NOT use the parallel (library names with 
# mp in them) version of the SGI BLAS.
#
BLAS_LIB       = -lblas ${FC_LIB}
LAPACK_LIB     = -lcomplib.sgimath
#BLAS_LIB     = /home/petsc/software/blaslapack/blas_IRIX64.a ${FC_LIB}
#LAPACK_LIB     = /home/petsc/software/blaslapack/lapack_IRIX64.a
#
# Location of X-windows software
#
X11_INCLUDE    = 
X11_LIB        = -lX11
#
# Location of MPI (Message Passing Interface) software  
#
# We recommend using SGI's MPI implementation over MPICH on the Origin and 
# Powerchallenge.
#
# If you are using the MPICH implementation of MPI with version BELOW 1.1,
# you should remove the -DPETSC_HAVE_INT_MPI_COMM. If you are using MPICH Version 1.1
# or SGI's version of MPI you MUST retain it.
#
MPI_LIB        = -lmpi
MPI_INCLUDE    = -DPETSC_HAVE_INT_MPI_COMM
MPIRUN         = /usr/sbin/npri -w /bin/mpirun
#
# The following lines can be used with MPICH
#
#MPI_LIB        = -L/home/petsc/mpich/lib/IRIX64/ch_p4 -lmpi
#MPI_INCLUDE    = -DPETSC_HAVE_INT_MPI_COMM -I/home/petsc/mpich/include
#MPIRUN         =  /home/petsc/mpich/lib/IRIX64/ch_p4/mpirun
#
# The following lines can be used with MPIUNI
#
#MPI_LIB         =${LDIR}/libmpiuni.a
#MPI_INCLUDE     = -I${PETSC_DIR}/src/sys/src/mpiuni -DPETSC_HAVE_INT_MPI_COMM
#MPIRUN          = ${PETSC_DIR}/src/sys/src/mpiuni/mpirun
#
# ----------------------------------------------------------------------------------------  
#  Locations of optional packages. Comment out those you do not have and change the 
#  values in PCONF below accordingly.
# ----------------------------------------------------------------------------------------  
#
# Optional location of MPE
# If using MPICH version 1.1.2 or higher use the flag -DPETSC_HAVE_MPE_INITIALIZED_LOGGING
#
#MPE_INCLUDE   = -I/usr/local/mpi/include
#MPE_LIB       = -L/usr/local/mpi/lib/IRIX64/ch_shmem -lmpe -lpmpi
#
# Optional location of BlockSolve (MPI version)
#
BS_INCLUDE = -I/home/petsc/software/BlockSolve95/include
BS_LIB     = -L/home/petsc/software/BlockSolve95/lib/libO/${PETSC_ARCH} -lBS95
#
# Optional Matlab location
#
#  The CFLAGS and following stuff is due to Matlab being miss-intalled on our machine
#  you may not need any of that stuff
MCC            = cc
#CMEX           = /home/bsmith/bin/IRIX64/mex CFLAGS='-32' LDFLAGS="-32 -mips4 -shared -U -Bsymbolic -exported_symbol mexFunction -exported_symbol mexVersion"
CMEX           = mex CFLAGS='-32' LDFLAGS="-32 -mips4 -shared -U -Bsymbolic -exported_symbol mexFunction -exported_symbol mexVersion"
MATLABCOMMAND  = matlab -sgi
#
#
# Option location where adiC is installed
#
#ADIC_INCLUDE = -I${ADIC}/run-lib/include -I${ADIC}/gradient -I${ADIC}/include -Dad_GRAD_MAX=1
#ADIC_LIB     = -L${ADIC}/lib -L${ADIC}/run-lib/lib -L${ADIC}/gradient -lADIntrinsics-C -laif_
#ADIC_CC      = adiC -aeud gradient -i ${PETSC_DIR}/bmake/adicmastercontrol
#
# Optional location of PVODE; Alan Hindmarsh's parallel ODE solver
# 
PVODE_INCLUDE = -I/home/petsc/software/MPI_PVODE/include
PVODE_LIB     = /home/petsc/software/MPI_PVODE/lib/IRIX64/libpvode.a
#
# Optional location of ParMetis
#
#PARMETIS_INCLUDE = -I/home/bsmith/libraries/ParMetis.v1.0
#PARMETIS_LIB     = /home/bsmith/libraries/ParMetis.v1.0/libparmetis.a 
#
#
#  Optional location for ALICE Memory Snooper
#
AMS_INCLUDE = -I/home/alice/ams/include
AMS_LIB     = -L/home/alice/ams/lib/libg/irix64 -lamspub -lamsutilmt -lamsutil
# ---------------------------------------------------------------------------------------
#
# PCONF - indicates which OPTIONAL external packages are available at your site
#
# If you have a package then make sure -DPETSC_HAVE_packagename is indicated below and 
# the locations are appropriately indicated above. If you do not have the package then
# comment out the locations indicated for that package above.
#
#PCONF         = -DPETSC_HAVE_BLOCKSOLVE 
PCONF        = -DPETSC_HAVE_BLOCKSOLVE  -DPETSC_HAVE_PVODE  -DPETSC_HAVE_AMS
#PCONF        = -DPETSC_HAVE_BLOCKSOLVE  -DPETSC_HAVE_PVODE
EXTERNAL_LIB  = ${BS_LIB}  ${MPE_LIB} ${PVODE_LIB} ${ADIC_LIB}  ${AMS_LIB}
#
# ---------------------------------------------------------------------------------------
#
# If you are using shared version of any external libraries you must make this
# point to the directories where all your shared libraries are stored.
#
C_DYLIBPATH     = ${CLINKER_SLFLAG}/home/petsc/software/BlockSolve95/lib/libO/IRIX64
F_DYLIBPATH     = ${FLINKER_SLFLAG}/home/petsc/software/BlockSolve95/lib/libO/IRIX64
